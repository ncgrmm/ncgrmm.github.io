[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Latest Posts",
    "section": "",
    "text": "Bayesian Inference and Factor Graphs\n\n\n\nprobabilistic-ml\n\nprobability-theory\n\n\n\n\n\n\n\n\n\nJul 29, 2025\n\n\nNico Grimm\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "In the future, you will find an about text here but as you can see, I haven’t written it yet."
  },
  {
    "objectID": "posts/factor_graphs/index.html#bayesian-inference-model",
    "href": "posts/factor_graphs/index.html#bayesian-inference-model",
    "title": "Bayesian Inference and Factor Graphs",
    "section": "Bayesian Inference Model",
    "text": "Bayesian Inference Model\nDerived from a fundamental result in probability theory, the Bayesian Inference Model is one of the basic concepts in probabilistic machine learning. It represents the elemental idea of updating prior beliefs over model parameters through data.  p(B \\: \\vert \\: A) = \\frac{p(A \\: \\vert \\: B)\\cdot p(B)}{p(A)}  \\tag{1} In machine learning, we typically have a model which can be specified by some set of parameters \\mathbf{\\Phi} deal with training data \\mathcal{D}. Usually, the parameters are directly updated by optimizing a given metric on the training data, e.g. minimizing a loss function. However, in this setting we are specifiying a so-called prior distribution over \\mathbf{\\Phi} and update the distribution through training. Not only does this allow to incorporate assumptions over possibly important model parameters, but also captures the model’s uncertainty of the weight’s values after training and makes it possible to compute the uncertainty of predictions.\nStarting off with Bayes’ Rule (see Equation 1), which relates the probability of B to the probability of B given A. By replacing B with \\mathbf{\\Phi} and A with \\mathcal{D} we end up with the basic formula of Bayesian Inference: p(\\mathbf{\\Phi} \\: \\vert \\: \\mathcal{D}) \\propto p(\\mathcal{D} \\: \\vert \\: \\mathbf{\\Phi}) \\cdot p(\\mathbf{\\Phi}) This is exactly the tool to achieve what was proposed earlier. From the prior belief over parameters p(\\mathbf{\\Phi}) we can obtain an updated distribution after seeing the data p(\\mathbf{\\Phi} \\: \\vert \\: \\mathcal{D}), called posterior distribution, by multiplying with p(\\mathcal{D} \\: \\vert \\: \\mathbf{\\Phi}). This term is called likelihood and is very important. It is the probability of the training data having been generated by the current parameter values. It basically represents the actual model in the equation, as it describes the probability of the model output being the target value given the current parameter values.\nNow what about p(\\mathcal{D})? This term is called model evidence and essentially captures to which extent the proposed model, including it’s prior assumptions, is capable of fitting the data, because in order to calculate it, we need to integrate the product of prior and likelihood over all possible parameter values. In practice, it is often ommited, because it is\n\ntypically very hard to compute exactly\nconstant for a given model\nunnecessary for parameter estimation of the posterior"
  },
  {
    "objectID": "posts/factor_graphs/index.html#factor-graphs",
    "href": "posts/factor_graphs/index.html#factor-graphs",
    "title": "Bayesian Inference and Factor Graphs",
    "section": "Factor Graphs",
    "text": "Factor Graphs"
  },
  {
    "objectID": "posts/factor_graphs/index.html#sum-product-algorithm",
    "href": "posts/factor_graphs/index.html#sum-product-algorithm",
    "title": "Bayesian Inference and Factor Graphs",
    "section": "Sum-Product-Algorithm",
    "text": "Sum-Product-Algorithm"
  },
  {
    "objectID": "posts/factor_graphs/index.html#introduction",
    "href": "posts/factor_graphs/index.html#introduction",
    "title": "Bayesian Inference and Factor Graphs",
    "section": "Introduction",
    "text": "Introduction"
  },
  {
    "objectID": "posts/factor_graphs/index.html",
    "href": "posts/factor_graphs/index.html",
    "title": "Bayesian Inference and Factor Graphs",
    "section": "",
    "text": "#| echo: false\np {\n  text-align: justify\n}"
  }
]